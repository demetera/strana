{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_URL = \"https://www.strana.news\"\n",
    "\n",
    "def get_page(nr):\n",
    "    text = requests.get(f'{MAIN_URL}/news/page-{nr}.html').text\n",
    "    soup = bs(text, 'html.parser')\n",
    "    articles = soup.select('.lenta-news.clearfix')\n",
    "    page_articles = []\n",
    "    for a in articles:\n",
    "        url = a.select('.title .article')[0]['href']\n",
    "        title = a.select('.title .article')[0].getText().strip()\n",
    "        page_articles.append({'url': url, 'title': title})\n",
    "    return page_articles\n",
    "\n",
    "def get_article(url : str) -> dict :\n",
    "    text = requests.get(url).text\n",
    "    soup = bs(text, 'html.parser')\n",
    "    # raw_meta = soup.select(\"script[type='application/ld+json']\")[0].get_text().replace(\"\\n\", \"\")\n",
    "    try:\n",
    "        article_main = soup.select('.articles')[0]\n",
    "        title = article_main.select('.article-title.article-edit .article')[0].getText()\n",
    "        datetime = article_main.select('.date span.strana-adate')[0]['data-time']\n",
    "        article_text = article_main.select('.article-text')[0]\n",
    "        try:\n",
    "            image_url = article_text.select('.article-image img')[0]['src']\n",
    "        except:\n",
    "            image_url = \"\"\n",
    "        try:\n",
    "            caption = article_text.select('.caption')[0].get_text().strip()\n",
    "        except:\n",
    "            caption = \"\"\n",
    "        text = article_text.select('#article-body p')\n",
    "        content = ''\n",
    "        for i in text:\n",
    "            content += i.get_text().strip() + \" \"\n",
    "        return {'title': title, 'url': url, 'image': image_url, 'caption': caption, 'content': content, 'datetime': datetime}\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shortened (errored) article\n",
    "\n",
    "def get_shortened_article(url):\n",
    "    text = requests.get(url).text\n",
    "    soup = bs(text, 'html.parser')\n",
    "    article_main = soup.select('.article')[0]\n",
    "    title = article_main.select('.article-title.article-edit')[0].getText().strip()\n",
    "\n",
    "    art_date = article_main.select('.date span.strana-adate')[0]['data-time']\n",
    "    art_time = article_main.select('.date')[0].getText().split(',')[0]\n",
    "    if art_time.startswith('\\n'):\n",
    "        art_time = \"11:00\"\n",
    "    datetime = art_date + \" \" +  art_time + ':00'\n",
    "    article_text = article_main.select('.article-text')[0]\n",
    "    try:\n",
    "        image_url = article_text.select('.article-image img')[0]['src']\n",
    "    except:\n",
    "        image_url = \"\"\n",
    "    try:\n",
    "        caption = article_text.select('.caption')[0].get_text().strip()\n",
    "    except:\n",
    "        caption = \"\"\n",
    "    text = article_text.select('#article-body p')\n",
    "    content = ''\n",
    "    for i in text:\n",
    "        content += i.get_text().strip() + \" \"\n",
    "    return {'title': title, 'url': url, 'image': image_url, 'caption': caption, 'content': content, 'datetime': datetime}\n",
    "\n",
    "# get_shortened_article('https://strana.news/news/145928-zlata-ohnevich-na-chm-2018-budet-bolet-za-ispaniju-a-irina-jusupova-za-ehipet.html')\n",
    "# get_shortened_article('https://strana.news/news/453886-itohi-671-dnja-vojny-v-ukraine.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 100 written\n"
     ]
    }
   ],
   "source": [
    "# OPTIONAL\n",
    "# Collects links and title\n",
    "\n",
    "# PROTECTION WITH THE FILE NAME - pages2.csv\n",
    "\n",
    "PAGE_FROM = 113\n",
    "\n",
    "f = open ('pages.csv','a')\n",
    "f.write(\"url,title\\n\")\n",
    "for ind, pagenr in enumerate(range(PAGE_FROM, 0, -1)):\n",
    "    articles_on_page = get_page(pagenr)\n",
    "    for a in articles_on_page:\n",
    "        f.write(f\"{MAIN_URL}{a['url']}\"+\",\"+a['title'].replace(\",\", \"\")+\"\\n\")\n",
    "    if pagenr % 100 == 0:\n",
    "        print(\"Page \" + str(pagenr) + \" written\")\n",
    "    time.sleep(0.2)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference is 0 articles\n"
     ]
    }
   ],
   "source": [
    "# OPTIONAL\n",
    "# CLEANING DUPLICATES IN PAGES.CSV file\n",
    "\n",
    "ds = pd.read_csv('./pages.csv')\n",
    "len1 = len(ds)\n",
    "ds.drop_duplicates(inplace=True)\n",
    "len2 = len(ds)\n",
    "print (f'Difference is {len1-len2} articles')\n",
    "if (len1 - len2) > 0:\n",
    "    ds.to_csv('./pages.csv', index=False)\n",
    "    print ('File updated')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".....\n",
      "Last item is :url      https://www.strana.news/news/458439-chto-prois...\n",
      "title    730-й день войны в Украине. Что происходит 23 ...\n",
      "Name: 108, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Fetches n articles urls from pages.csv file and appends to the file\n",
    "# Headers to be included if NEW FILE\n",
    "\n",
    "# Process original and error pages (links, error_links, art in loop)\n",
    "\n",
    "ITERS = 10000\n",
    "\n",
    "FILE = './strana_articles.csv'\n",
    "\n",
    "header = ['title','url', 'image', 'caption', 'content', 'datetime']\n",
    "links = pd.read_csv('pages.csv') # PROCESS ORIGINAL PAGES\n",
    "#links = pd.read_csv('./strana_errors.csv') # PROCESS ERROR PAGES\n",
    "\n",
    "error_links = open('./strana_errors.csv', 'a') # IF ORIGINAL FILE PROCESSED\n",
    "#error_links = open('./strana_errors2.csv', 'a') # IF ERROR FILE PROCESSED\n",
    "\n",
    "with open(FILE, 'a') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=header)\n",
    "    writer.writeheader()\n",
    "    for ind, row in links.iterrows():\n",
    "        art = get_article(row['url']) # ORIGINAL ARTICLES\n",
    "        #art = get_shortened_article(row['url']) # ERROR ARTICLES PROCESSED\n",
    "        try:\n",
    "            writer.writerow({'title': art['title'], 'url': art['url'], 'image': art['image'], 'caption': art['caption'],\n",
    "                             'content': art['content'], 'datetime': art['datetime']})\n",
    "        except:\n",
    "            error_links.write(f\"{row['url']},{row['title']}\\n\")\n",
    "            pass\n",
    "        if ind == ITERS:\n",
    "            break\n",
    "        if ind % 20 == 0 and ind % 200 != 0 and ind % 1000 != 0 and ind !=0:\n",
    "            print(\".\", end='')\n",
    "        if ind % 200 == 0 and ind % 1000 !=0 and ind != 0:\n",
    "            print(\"|\", end='')\n",
    "        if ind % 1000 == 0 and ind != 0:\n",
    "            print (str(ind), end='\\n')\n",
    "        time.sleep(0.5) # set half a second instead of 0.25\n",
    "\n",
    "print (f\"\\nLast item is :{row}\")\n",
    "error_links.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_links.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download missing content to pages.csv\n",
    "# parse to strana_articles_temp.csv\n",
    "# inspect duplicated items and move them to articles.csv\n",
    "# consider to separate the dataset into multiple file\n",
    "# consider to process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/strana_2018.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22735"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RECOGNIZE EMPTY & ERRATIC SHIT\n",
    "counter = 0\n",
    "for item in range(len(df)):\n",
    "    if len(df.iloc[item]['datetime'].split()) < 2 :\n",
    "            counter += 1\n",
    "            # print (df.iloc[item]['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[9]['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.drop_duplicates(subset=['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
